{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Earth observation data cubes from GDAL image collections Introduction gdalcubes is a library to represent collections of Earth Observation (EO) images as on-demand data cubes (or multidimensional arrays ). Users define data cubes by spatiotemporal extent, resolution, and spatial reference system and let gdalcubes read only relevant parts of the data and simultaneously apply reprojection, resampling, and cropping (using gdalwarp ). Data cubes may be simply exported as NetCDF files or directly streamed chunk-wise into external software such as R or Python. The library furthermore implements simple operations to reduce data cubes over time, to apply pixel-wise arithmetic expressions, and to filter by space, time, and bands. gdalcubes is not a database, i.e., it does not need to store additional copies of the imagery but instead simply links to and indexes existing files / GDAL datasets, i.e. it can also directly access data in cloud environments with GDAL virtual file systems . The library is written in C++ and includes a command line interface as well as a package for R. A python package is planned for the future. gdalcubes is licensed under the Apache License 2.0 . Features Create image collections that link to and index existing imagery from local files or cloud storage Read multitemporal, multispectral image collections as on demand data cubes with desired spatiotemporal resolution, extent, and map projection Abstract from complexities in the data like different map projections for adjacent images and different resolutions for different spectral bands Stream chunks of data cubes to external programs (e.g. R, python) Scale computations on data cubes in distributed environments with gdalcubes_server and Docker (experimental) Warning The library is still in an early development version. Major changes are possible to make gdalcubes more user-friendly, more stable, faster, and more robust. The documentation is also preliminary and far from being complete.","title":"Home"},{"location":"index.html#earth-observation-data-cubes-from-gdal-image-collections","text":"","title":"Earth observation data cubes from GDAL image collections"},{"location":"index.html#introduction","text":"gdalcubes is a library to represent collections of Earth Observation (EO) images as on-demand data cubes (or multidimensional arrays ). Users define data cubes by spatiotemporal extent, resolution, and spatial reference system and let gdalcubes read only relevant parts of the data and simultaneously apply reprojection, resampling, and cropping (using gdalwarp ). Data cubes may be simply exported as NetCDF files or directly streamed chunk-wise into external software such as R or Python. The library furthermore implements simple operations to reduce data cubes over time, to apply pixel-wise arithmetic expressions, and to filter by space, time, and bands. gdalcubes is not a database, i.e., it does not need to store additional copies of the imagery but instead simply links to and indexes existing files / GDAL datasets, i.e. it can also directly access data in cloud environments with GDAL virtual file systems . The library is written in C++ and includes a command line interface as well as a package for R. A python package is planned for the future. gdalcubes is licensed under the Apache License 2.0 .","title":"Introduction"},{"location":"index.html#features","text":"Create image collections that link to and index existing imagery from local files or cloud storage Read multitemporal, multispectral image collections as on demand data cubes with desired spatiotemporal resolution, extent, and map projection Abstract from complexities in the data like different map projections for adjacent images and different resolutions for different spectral bands Stream chunks of data cubes to external programs (e.g. R, python) Scale computations on data cubes in distributed environments with gdalcubes_server and Docker (experimental)","title":"Features"},{"location":"index.html#warning","text":"The library is still in an early development version. Major changes are possible to make gdalcubes more user-friendly, more stable, faster, and more robust. The documentation is also preliminary and far from being complete.","title":"Warning"},{"location":"R.html","text":"The gdalcubes R package The gdalcubes R package is hosted at https://github.com/appelmar/gdalcubes_R. It is not available from CRAN and at the moment must be installed from sources. Installation You can install the package directly from GitHub using the devtools package . However, since the package includes the gdalcubes C++ library as a git submodule in its src folder, you must install the git command line client before. On Windows, you will furthermore need Rtools . Package building on Windows automatically downloads needed dependencies ( GDAL , NetCDF , SQLite , curl ) from rwinlib . On Linux, please install these libraries e.g. using your package manager. The following R code will then install the gdalcubes R package. library(devtools) install_git( https://github.com/appelmar/gdalcubes_R , args= --recursive ) Getting started The package includes a vignette that illustrates the basic concepts and funcitonality on a small ( 1 GB) MODIS dataset (see vignettes/getting_started.Rmd ).","title":"R package"},{"location":"R.html#the-gdalcubes-r-package","text":"The gdalcubes R package is hosted at https://github.com/appelmar/gdalcubes_R. It is not available from CRAN and at the moment must be installed from sources.","title":"The gdalcubes R package"},{"location":"R.html#installation","text":"You can install the package directly from GitHub using the devtools package . However, since the package includes the gdalcubes C++ library as a git submodule in its src folder, you must install the git command line client before. On Windows, you will furthermore need Rtools . Package building on Windows automatically downloads needed dependencies ( GDAL , NetCDF , SQLite , curl ) from rwinlib . On Linux, please install these libraries e.g. using your package manager. The following R code will then install the gdalcubes R package. library(devtools) install_git( https://github.com/appelmar/gdalcubes_R , args= --recursive )","title":"Installation"},{"location":"R.html#getting-started","text":"The package includes a vignette that illustrates the basic concepts and funcitonality on a small ( 1 GB) MODIS dataset (see vignettes/getting_started.Rmd ).","title":"Getting started"},{"location":"concepts.html","text":"Concepts This page describes the basic concepts and terms used by gdalcubes. Some of the definitions below may vary from other libraries. Image collections An image collection is simply a set of images where all images contain identical variables ( bands ). Images have a spatial footprint and a recording date/time. Values of the bands from one image may be stored in a single or in several files and bands may also differ in their spatial resolution. Two different images in a collection may have different map projections. The figure below illustrates the basic structure: the image collection has 3 images, images each have a spatial extent, recording date/time, and a map projection and in this case 9 bands, where the band data come from three files per image. For example, Landsat imagery comes in a very simple format where one image consist of a set of GeoTIFF files with a one to one relationship between files and bands. MODIS data in contrast come as one HDF4 file per image that contains all bands. In gdalcubes, anything that is readable by GDAL may contain actual band data of an image. Typically, this will be files, but actually can be anything that the gdalinfo command understands, including cloud storage, databases, and archive files (see below) through GDAL virtual file systems . Examples for valid GDAL dataset references include /vsizip/archive.zip/xyz.tif (a GeoTIFF file in a .zip archive), test.tif (a simple local GeoTIFF file), SENTINEL2_L1C:S2A_OPER_MTD_SAFL1C_PDMC_20150818T101440_R022_V20150813T102406_20150813T102406.xml:10m:EPSG_32632 (higher level GDAL Sentinel 2 datasets), /vsicurl/https://download.osgeo.org/geotiff/samples/spot/chicago/UTM2GTIF.TIF (file on an HTTP server). Instead of files we therefore often use the terms GDAL dataset reference or GDAL dataset descriptor . Images, bands, and GDAL dataset references are indexed by gdalcubes in a single image collection file. This file is a simple SQLite database, which stores links to all GDAL dataset references and how they relate to images and bands. To allow fast filtering, the collection file additionally stores the spatial extent, the aquisition date/time, and the spatial reference system of images. Further imagary metadata is currently not included but future releases will do so, e.g. to filter images in a collection by cloud cover. gdalcubes comes with a function to create an image collection from a set of GDAL dataset references as strings. This function however, must know some details about the structure of a specific EO data product: How to derive date/time from images? Which GDAL dataset references include data for specific bands / variables? Which GDAL dataset references belong to the same image? These questions are answered as a set of regular expressions on the GDAL dataset references in an image collection format . Image collection formats are JSON descriptions of how the data is structured. gdalcubes comes with some predefined formats. An example format for Sentinel 2 level 1C data can is presented below. { description : Image collection format for Sentinel 2 Level 1C data as downloaded from the Copernicus Open Access Hub, expects a list of file paths as input. The format works on original ZIP compressed as well as uncompressed imagery. , tags : [ Sentinel , Copernicus , ESA , TOA ], pattern : .+/IMG_DATA/.+\\\\.jp2 , images : { pattern : .*/(.+)\\\\.SAFE.* }, datetime : { pattern : .*MSIL1C_(.+?)_.* , format : %Y%m%dT%H%M%S }, bands : { B01 : { nodata : 0, pattern : .+_B01\\\\.jp2 }, B02 : { nodata : 0, pattern : .+_B02\\\\.jp2 }, B03 : { nodata : 0, pattern : .+_B03\\\\.jp2 }, B04 : { nodata : 0, pattern : .+_B04\\\\.jp2 }, B05 : { nodata : 0, pattern : .+_B05\\\\.jp2 }, B06 : { nodata : 0, pattern : .+_B06\\\\.jp2 }, B07 : { nodata : 0, pattern : .+_B07\\\\.jp2 }, B08 : { nodata : 0, pattern : .+_B08\\\\.jp2 }, B8A : { nodata : 0, pattern : .+_B8A\\\\.jp2 }, B09 : { nodata : 0, pattern : .+_B09\\\\.jp2 }, B10 : { nodata : 0, pattern : .+_B10\\\\.jp2 }, B11 : { nodata : 0, pattern : .+_B11\\\\.jp2 }, B12 : { nodata : 0, pattern : .+_B12\\\\.jp2 } } } gdalcubes then automatically creates the image collection file from a set of input datasets and a collection format. A few predefined collection formats are included in the library (see https://github.com/appelmar/gdalcubes/tree/master/formats). Data cubes Data cubes are multidimensional arrays with dimensions band, datetime, y (latitude / northing), and x (longitude / easting). Cells of a cube all have the same size in space and time with regard to a defined spatial reference system. Data cubes are different from image collections. Image collections are irregular in time, may contain gaps, and do not have a globally valid projection. To create data cubes from image collections, we define a data cube view (or simply view ). Data cube views convert an image collection to a data cube by defining the basic shape of the cube, i.e. how we look at the data from an image collection. The data cube view includes the spatiotemporal extent, the spatial reference system / map projection, the spatiotemporal resolution either by number of or by the size of cells, a resampling algorithm used in gdalwarp , and an aggregation algorithm that combines values of several images if they are located in the same cell of the target cube Views can be serialized as simple JSON object as in the example below. Note that there is no single correct view for a specific image collections. Instead, they are useful e.g. to run analysis an small subsets during model development before running on the full-resolution data. { aggregation : min , resampling : bilinear , space : { left : 22.9, right : 23.1, top : -18.9, bottom : -19.1, proj : EPSG:4326 , nx : 500, ny : 500 }, time : { t0 : 2017-01-01 , t1 : 2018-01-01 , dt : P1M } } Data from images in a collection are read on-the-fly with regard to a specific data cube view. The target data cube is read chunk-wise, where chunk sizes in all directions can be defined by the user. The procedure to read data of one chunk is the following: Find all GDAL datasets of the collection that are located within the spatiotemporal extent of the chunk Iterate over all found datasets and do the following steps: Apply gdalwarp to crop, reproject / transform, and resample the current dataset according to the spatiotemporal extent of the current chunk and the data cube view. Store the result as an in-memory GDAL dataset . Copy the result to the in-memory chunk buffer (a four-dimensional array) at the correct temporal slice and the correct bands. If the chunk buffer already contains values at the target position, feed a pixel-wise aggregator (e.g. mean, median, min, max ) to combine pixel values from multiple images which are located at the same cell in the data cube. Finalize the pixel-wise aggregator if needed (e.g. divide pixel values by n n for mean aggregation). Internally, chunk buffers are arrays of type double. Image data is however read according to their orgininal data type. gdalwarp does the type conversion automatically, i.e. only the size of the chunk buffer in memory is larger but not the data that is transferred over the network for remotely stored imagery. If input images contain lower-resolution overviews, these are used automatically by gdalwarp depending on the target resolution of the cube. Operations on data cubes Currently, gdalcubes includes operations on data cubes to select bands, apply pixel wise arithmetic expressions, reduce data cubes over time, join identically shaped cubes, and stream chunks of data cubes to external software (such as R or Python). Internally, all of the processes inherit from the same data cube type but take one or more existing data cubes as input arguments. Once data cube operation objects are created, only the shape of the result cube will be derived but no values will be calculated or read from input cubes. Data cube objects are primarily proxy objects and follow the concept of lazy evaluation. This also applies to chains of operations. Data cube objects can be serialized as a simple JSON formatted directed acyclich graph. Below, a data cube operation chain that selects two bands of an image collection, computes differences between the bands, and finally computes the median differences over time is serialized as a graph. This graph can be used to recreate the same operation chain. { cube_type : reduce , in_cube : { band_names : [ LST difference ], cube_type : apply_pixel , expr : [ 0.02*(LST_DAY-LST_NIGHT) ], in_cube : { bands : [ LST_DAY , LST_NIGHT ], cube_type : select_bands , in_cube : { chunk_size : [ 16, 256, 256 ], cube_type : image_collection , file : MOD11A2.db , view : { aggregation : first , resampling : near , space : { bottom : 4447802.0, left : -1703607.0, nx : 633, ny : 413, proj : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs , right : 1703607.0, top : 6671703.0 }, time : { dt : P1M , t0 : 2018-01 , t1 : 2018-12 } } } } }, reducer : median } Distributed data cube processing The library comes with a server executable gdalcubes_server that listens on a given port for incoming HTTP requests to a REST-like API. This feature is still highly experimental. The general idea is that: Clients define a swarm of gdalcubes server instances that are used to evaluate a data cube operation. Processing of the result cube is then distributed to swarm members by chunks. The client shares its execution context, i.e. all files within the working directory by uploading files to all swarm members. GDAL dataset references in image collection files must be globally valid (i.e. accessible for all swarm members). Though this is a strong assumption, it should work well for cloud processing where imagery has global object identifers (such as S3 buckets). Details of the exposed API can be found at https://appelmar.github.io/gdalcubes/server-api.html.","title":"Basic concepts"},{"location":"concepts.html#concepts","text":"This page describes the basic concepts and terms used by gdalcubes. Some of the definitions below may vary from other libraries.","title":"Concepts"},{"location":"concepts.html#image-collections","text":"An image collection is simply a set of images where all images contain identical variables ( bands ). Images have a spatial footprint and a recording date/time. Values of the bands from one image may be stored in a single or in several files and bands may also differ in their spatial resolution. Two different images in a collection may have different map projections. The figure below illustrates the basic structure: the image collection has 3 images, images each have a spatial extent, recording date/time, and a map projection and in this case 9 bands, where the band data come from three files per image. For example, Landsat imagery comes in a very simple format where one image consist of a set of GeoTIFF files with a one to one relationship between files and bands. MODIS data in contrast come as one HDF4 file per image that contains all bands. In gdalcubes, anything that is readable by GDAL may contain actual band data of an image. Typically, this will be files, but actually can be anything that the gdalinfo command understands, including cloud storage, databases, and archive files (see below) through GDAL virtual file systems . Examples for valid GDAL dataset references include /vsizip/archive.zip/xyz.tif (a GeoTIFF file in a .zip archive), test.tif (a simple local GeoTIFF file), SENTINEL2_L1C:S2A_OPER_MTD_SAFL1C_PDMC_20150818T101440_R022_V20150813T102406_20150813T102406.xml:10m:EPSG_32632 (higher level GDAL Sentinel 2 datasets), /vsicurl/https://download.osgeo.org/geotiff/samples/spot/chicago/UTM2GTIF.TIF (file on an HTTP server). Instead of files we therefore often use the terms GDAL dataset reference or GDAL dataset descriptor . Images, bands, and GDAL dataset references are indexed by gdalcubes in a single image collection file. This file is a simple SQLite database, which stores links to all GDAL dataset references and how they relate to images and bands. To allow fast filtering, the collection file additionally stores the spatial extent, the aquisition date/time, and the spatial reference system of images. Further imagary metadata is currently not included but future releases will do so, e.g. to filter images in a collection by cloud cover. gdalcubes comes with a function to create an image collection from a set of GDAL dataset references as strings. This function however, must know some details about the structure of a specific EO data product: How to derive date/time from images? Which GDAL dataset references include data for specific bands / variables? Which GDAL dataset references belong to the same image? These questions are answered as a set of regular expressions on the GDAL dataset references in an image collection format . Image collection formats are JSON descriptions of how the data is structured. gdalcubes comes with some predefined formats. An example format for Sentinel 2 level 1C data can is presented below. { description : Image collection format for Sentinel 2 Level 1C data as downloaded from the Copernicus Open Access Hub, expects a list of file paths as input. The format works on original ZIP compressed as well as uncompressed imagery. , tags : [ Sentinel , Copernicus , ESA , TOA ], pattern : .+/IMG_DATA/.+\\\\.jp2 , images : { pattern : .*/(.+)\\\\.SAFE.* }, datetime : { pattern : .*MSIL1C_(.+?)_.* , format : %Y%m%dT%H%M%S }, bands : { B01 : { nodata : 0, pattern : .+_B01\\\\.jp2 }, B02 : { nodata : 0, pattern : .+_B02\\\\.jp2 }, B03 : { nodata : 0, pattern : .+_B03\\\\.jp2 }, B04 : { nodata : 0, pattern : .+_B04\\\\.jp2 }, B05 : { nodata : 0, pattern : .+_B05\\\\.jp2 }, B06 : { nodata : 0, pattern : .+_B06\\\\.jp2 }, B07 : { nodata : 0, pattern : .+_B07\\\\.jp2 }, B08 : { nodata : 0, pattern : .+_B08\\\\.jp2 }, B8A : { nodata : 0, pattern : .+_B8A\\\\.jp2 }, B09 : { nodata : 0, pattern : .+_B09\\\\.jp2 }, B10 : { nodata : 0, pattern : .+_B10\\\\.jp2 }, B11 : { nodata : 0, pattern : .+_B11\\\\.jp2 }, B12 : { nodata : 0, pattern : .+_B12\\\\.jp2 } } } gdalcubes then automatically creates the image collection file from a set of input datasets and a collection format. A few predefined collection formats are included in the library (see https://github.com/appelmar/gdalcubes/tree/master/formats).","title":"Image collections"},{"location":"concepts.html#data-cubes","text":"Data cubes are multidimensional arrays with dimensions band, datetime, y (latitude / northing), and x (longitude / easting). Cells of a cube all have the same size in space and time with regard to a defined spatial reference system. Data cubes are different from image collections. Image collections are irregular in time, may contain gaps, and do not have a globally valid projection. To create data cubes from image collections, we define a data cube view (or simply view ). Data cube views convert an image collection to a data cube by defining the basic shape of the cube, i.e. how we look at the data from an image collection. The data cube view includes the spatiotemporal extent, the spatial reference system / map projection, the spatiotemporal resolution either by number of or by the size of cells, a resampling algorithm used in gdalwarp , and an aggregation algorithm that combines values of several images if they are located in the same cell of the target cube Views can be serialized as simple JSON object as in the example below. Note that there is no single correct view for a specific image collections. Instead, they are useful e.g. to run analysis an small subsets during model development before running on the full-resolution data. { aggregation : min , resampling : bilinear , space : { left : 22.9, right : 23.1, top : -18.9, bottom : -19.1, proj : EPSG:4326 , nx : 500, ny : 500 }, time : { t0 : 2017-01-01 , t1 : 2018-01-01 , dt : P1M } } Data from images in a collection are read on-the-fly with regard to a specific data cube view. The target data cube is read chunk-wise, where chunk sizes in all directions can be defined by the user. The procedure to read data of one chunk is the following: Find all GDAL datasets of the collection that are located within the spatiotemporal extent of the chunk Iterate over all found datasets and do the following steps: Apply gdalwarp to crop, reproject / transform, and resample the current dataset according to the spatiotemporal extent of the current chunk and the data cube view. Store the result as an in-memory GDAL dataset . Copy the result to the in-memory chunk buffer (a four-dimensional array) at the correct temporal slice and the correct bands. If the chunk buffer already contains values at the target position, feed a pixel-wise aggregator (e.g. mean, median, min, max ) to combine pixel values from multiple images which are located at the same cell in the data cube. Finalize the pixel-wise aggregator if needed (e.g. divide pixel values by n n for mean aggregation). Internally, chunk buffers are arrays of type double. Image data is however read according to their orgininal data type. gdalwarp does the type conversion automatically, i.e. only the size of the chunk buffer in memory is larger but not the data that is transferred over the network for remotely stored imagery. If input images contain lower-resolution overviews, these are used automatically by gdalwarp depending on the target resolution of the cube.","title":"Data cubes"},{"location":"concepts.html#operations-on-data-cubes","text":"Currently, gdalcubes includes operations on data cubes to select bands, apply pixel wise arithmetic expressions, reduce data cubes over time, join identically shaped cubes, and stream chunks of data cubes to external software (such as R or Python). Internally, all of the processes inherit from the same data cube type but take one or more existing data cubes as input arguments. Once data cube operation objects are created, only the shape of the result cube will be derived but no values will be calculated or read from input cubes. Data cube objects are primarily proxy objects and follow the concept of lazy evaluation. This also applies to chains of operations. Data cube objects can be serialized as a simple JSON formatted directed acyclich graph. Below, a data cube operation chain that selects two bands of an image collection, computes differences between the bands, and finally computes the median differences over time is serialized as a graph. This graph can be used to recreate the same operation chain. { cube_type : reduce , in_cube : { band_names : [ LST difference ], cube_type : apply_pixel , expr : [ 0.02*(LST_DAY-LST_NIGHT) ], in_cube : { bands : [ LST_DAY , LST_NIGHT ], cube_type : select_bands , in_cube : { chunk_size : [ 16, 256, 256 ], cube_type : image_collection , file : MOD11A2.db , view : { aggregation : first , resampling : near , space : { bottom : 4447802.0, left : -1703607.0, nx : 633, ny : 413, proj : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs , right : 1703607.0, top : 6671703.0 }, time : { dt : P1M , t0 : 2018-01 , t1 : 2018-12 } } } } }, reducer : median }","title":"Operations on data cubes"},{"location":"concepts.html#distributed-data-cube-processing","text":"The library comes with a server executable gdalcubes_server that listens on a given port for incoming HTTP requests to a REST-like API. This feature is still highly experimental. The general idea is that: Clients define a swarm of gdalcubes server instances that are used to evaluate a data cube operation. Processing of the result cube is then distributed to swarm members by chunks. The client shares its execution context, i.e. all files within the working directory by uploading files to all swarm members. GDAL dataset references in image collection files must be globally valid (i.e. accessible for all swarm members). Though this is a strong assumption, it should work well for cloud processing where imagery has global object identifers (such as S3 buckets). Details of the exposed API can be found at https://appelmar.github.io/gdalcubes/server-api.html.","title":"Distributed data cube processing"},{"location":"cpp-api.html","text":"this file is a placeholder for documentation build by doxygen","title":"C++ API"},{"location":"credits.html","text":"Credits gdalcubes wouldn't exist without all the great effort of other open-source projects. This document presents a list of used third-party software including their purpose, copyright, and licensing information in no particular order. Please notice that some libaries are only used by the command line client or gdalcubes_server, but not by the core library. GDAL : A translator library for raster and vector geospatial data formats Copyright (c) 2000, Frank Warmerdam Copyright (c) The GDAL/OGR project team License: MIT license Parts of GDAL are licensed under different terms, see https://github.com/OSGeo/gdal/blob/master/gdal/LICENSE.TXT gdalcubes may statically or dynamically link to the gdal library depending on compilation flags json : JSON for Modern C++ Copyright (c) 2013-2018 Niels Lohmann License: MIT license gdalcubes distributes an unmodified version of the library under src/external/json.hpp SQLite : A self-contained, high-reliability, embedded, full-featured, public-domain, SQL database engine Copyright: public domain License: public domain gdalcubes may statically or dynamically link to the sqlite library depending on compilation flags CURL : Command line tool and library for transferring data with URLs Copyright (c) 1996 - 2018, Daniel Stenberg License: curl license gdalcubes may statically or dynamically link to the libcurl library depending on compilation flags ExprTk : A C++ Mathematical Expression Parsing and Evaluation Library Copyright (c) Arash Partow (1999-2018) License: MIT license gdalcubes distributes an unmodified version of the library under src/external/exprtk.hpp TinyExpr : A very small recursive descent parser and evaluation engine for math expressions Copyright (c) 2015-2018 Lewis Van Winkle License: zlib license gdalcubes distributes an unmodified version of the library under src/external/tinyexpr netCDF : The Unidata network Common Data Form C library Copyright (c) 1993-2017 University Corporation for Atmospheric Research/Unidata License: MIT-like, see https://www.unidata.ucar.edu/software/netcdf/copyright.html DOI: http://doi.org/10.5065/D6H70CW6 gdalcubes may statically or dynamically link to the NetCDF C library depending on compilation flags tiny-process-library : A small platform independent library making it simple to create and stop new processes in C++ Copyright (c) 2015-2018 Ole Christian Eidheim License: MIT license gdalcubes_server includes may statically or dynamically link to the cpprestsdk library depending on compilation flags gdalcubes distributes an unmodified version of the library under src/external/tiny-process-library Catch2 : A modern, C++-native, header-only, test framework for unit-tests, TDD and BDD Copyright (c) 2010 Two Blue Cubes Ltd License: Boost Software License 1.0 gdalcubes distributes an unmodified version of the library under src/external/catch.hpp Boost.Filesystem Copyright (c) Beman Dawes, 2011 License: Boost Software License 1.0 gdalcubes may statically or dynamically link to the Boost.Filesystem library depending on compilation flags Boost.Program_options Copyright (c) 2002-2004 Vladimir Prus License: Boost Software License 1.0 gdalcubes may statically or dynamically link to the Boost.Program_options library depending on compilation flags Date : A date and time library based on the C++11/14/17 header Copyright (c) 2015, 2016, 2017 Howard Hinnant Copyright (c) 2016 Adrian Colomitchi Copyright (c) 2017 Florian Dang Copyright (c) 2017 Paul Thompson Copyright (c) 2018 Tomasz Kami\u0144ski License: MIT license gdalcubes distributes an unmodified version of the library under src/external/date.h cpprestsdk Copyright (c) Microsoft Corporation License: MIT license gdalcubes_server includes may statically or dynamically link to the cpprestsdk library depending on compilation flags Derived work such as R or Python packages may use further external software (see their documentation).","title":"Credits"},{"location":"credits.html#credits","text":"gdalcubes wouldn't exist without all the great effort of other open-source projects. This document presents a list of used third-party software including their purpose, copyright, and licensing information in no particular order. Please notice that some libaries are only used by the command line client or gdalcubes_server, but not by the core library. GDAL : A translator library for raster and vector geospatial data formats Copyright (c) 2000, Frank Warmerdam Copyright (c) The GDAL/OGR project team License: MIT license Parts of GDAL are licensed under different terms, see https://github.com/OSGeo/gdal/blob/master/gdal/LICENSE.TXT gdalcubes may statically or dynamically link to the gdal library depending on compilation flags json : JSON for Modern C++ Copyright (c) 2013-2018 Niels Lohmann License: MIT license gdalcubes distributes an unmodified version of the library under src/external/json.hpp SQLite : A self-contained, high-reliability, embedded, full-featured, public-domain, SQL database engine Copyright: public domain License: public domain gdalcubes may statically or dynamically link to the sqlite library depending on compilation flags CURL : Command line tool and library for transferring data with URLs Copyright (c) 1996 - 2018, Daniel Stenberg License: curl license gdalcubes may statically or dynamically link to the libcurl library depending on compilation flags ExprTk : A C++ Mathematical Expression Parsing and Evaluation Library Copyright (c) Arash Partow (1999-2018) License: MIT license gdalcubes distributes an unmodified version of the library under src/external/exprtk.hpp TinyExpr : A very small recursive descent parser and evaluation engine for math expressions Copyright (c) 2015-2018 Lewis Van Winkle License: zlib license gdalcubes distributes an unmodified version of the library under src/external/tinyexpr netCDF : The Unidata network Common Data Form C library Copyright (c) 1993-2017 University Corporation for Atmospheric Research/Unidata License: MIT-like, see https://www.unidata.ucar.edu/software/netcdf/copyright.html DOI: http://doi.org/10.5065/D6H70CW6 gdalcubes may statically or dynamically link to the NetCDF C library depending on compilation flags tiny-process-library : A small platform independent library making it simple to create and stop new processes in C++ Copyright (c) 2015-2018 Ole Christian Eidheim License: MIT license gdalcubes_server includes may statically or dynamically link to the cpprestsdk library depending on compilation flags gdalcubes distributes an unmodified version of the library under src/external/tiny-process-library Catch2 : A modern, C++-native, header-only, test framework for unit-tests, TDD and BDD Copyright (c) 2010 Two Blue Cubes Ltd License: Boost Software License 1.0 gdalcubes distributes an unmodified version of the library under src/external/catch.hpp Boost.Filesystem Copyright (c) Beman Dawes, 2011 License: Boost Software License 1.0 gdalcubes may statically or dynamically link to the Boost.Filesystem library depending on compilation flags Boost.Program_options Copyright (c) 2002-2004 Vladimir Prus License: Boost Software License 1.0 gdalcubes may statically or dynamically link to the Boost.Program_options library depending on compilation flags Date : A date and time library based on the C++11/14/17 header Copyright (c) 2015, 2016, 2017 Howard Hinnant Copyright (c) 2016 Adrian Colomitchi Copyright (c) 2017 Florian Dang Copyright (c) 2017 Paul Thompson Copyright (c) 2018 Tomasz Kami\u0144ski License: MIT license gdalcubes distributes an unmodified version of the library under src/external/date.h cpprestsdk Copyright (c) Microsoft Corporation License: MIT license gdalcubes_server includes may statically or dynamically link to the cpprestsdk library depending on compilation flags Derived work such as R or Python packages may use further external software (see their documentation).","title":"Credits"},{"location":"installation.html","text":"Installation Installation from sources Linux gdalcubes can be compiled from sources via CMake . CMake automatically checks for mandatory and optional dependencies and adapts the build configuration. The following commands install gdalcubes from sources. git clone https://github.com/appelmar/gdalcubes cd gdalcubes mkdir -p build cd build cmake -DCMAKE_BUILD_TYPE=Release ../ make sudo make install If any of required libraries are not available on your system, please use your package manager to install these before. Using Ubuntu, sudo apt-get install libgdal-dev libnetcdf-dev libcurl4-openssl-dev libsqlite3-dev will install the libraries needed to compile the core gdalcubes library. If you want to compile the command line interface, you will furthermore need sudo apt-get install libboost-program-options-dev libboost-system-dev and running gdalcubes as a server additionally requires sudo apt-get install libcpprest-dev . Windows The core library has been successfully compiled with a MinGW-w64 toolchain. The R package on Windows e.g. links to prebuild dependencies from rwinlib . Detailed instructions will be added soon. To use the R package, however, you do not need to build the gdalcubes library before. We have not yet tried to compile gdalcubes with Microsoft Visual Studio. Docker The Dockerfile at the root of the project is built on a minimal Ubuntu installation but installs all dependencies and compiles gdalcubes from sources automatically. git clone https://github.com/appelmar/gdalcubes cd gdalcubes docker build -t appelmar/gdalcubes . docker run -d -p 11111:1111 appelmar/gdalcubes # runs gdalcubes_server as a deamon docker run appelmar/gdalcubes /bin/bash # get a command line where you can run gdalcubes","title":"Installation"},{"location":"installation.html#installation","text":"","title":"Installation"},{"location":"installation.html#installation-from-sources","text":"","title":"Installation from sources"},{"location":"installation.html#linux","text":"gdalcubes can be compiled from sources via CMake . CMake automatically checks for mandatory and optional dependencies and adapts the build configuration. The following commands install gdalcubes from sources. git clone https://github.com/appelmar/gdalcubes cd gdalcubes mkdir -p build cd build cmake -DCMAKE_BUILD_TYPE=Release ../ make sudo make install If any of required libraries are not available on your system, please use your package manager to install these before. Using Ubuntu, sudo apt-get install libgdal-dev libnetcdf-dev libcurl4-openssl-dev libsqlite3-dev will install the libraries needed to compile the core gdalcubes library. If you want to compile the command line interface, you will furthermore need sudo apt-get install libboost-program-options-dev libboost-system-dev and running gdalcubes as a server additionally requires sudo apt-get install libcpprest-dev .","title":"Linux"},{"location":"installation.html#windows","text":"The core library has been successfully compiled with a MinGW-w64 toolchain. The R package on Windows e.g. links to prebuild dependencies from rwinlib . Detailed instructions will be added soon. To use the R package, however, you do not need to build the gdalcubes library before. We have not yet tried to compile gdalcubes with Microsoft Visual Studio.","title":"Windows"},{"location":"installation.html#docker","text":"The Dockerfile at the root of the project is built on a minimal Ubuntu installation but installs all dependencies and compiles gdalcubes from sources automatically. git clone https://github.com/appelmar/gdalcubes cd gdalcubes docker build -t appelmar/gdalcubes . docker run -d -p 11111:1111 appelmar/gdalcubes # runs gdalcubes_server as a deamon docker run appelmar/gdalcubes /bin/bash # get a command line where you can run gdalcubes","title":"Docker"},{"location":"server-api.html","text":"REST-like web API for distributed processing For distributed processing, gdalcubes and gdalcubes_server communicate via a REST-like API (no HATEOAS). The following documentation describes available endpoints. By default, gdalcubes_server will listen on http://0.0.0.0:1111/gdalcubes/api/ . GET /version Returns version information as text HEAD /file?name=xyz size=1234 Checks whether the server instance has file with name and size equal to the given query parameters. Returns 200, if the file is available, 409 if a file with the same name has different size, ... POST /file?name=xyz Uploads the file in the application/octet-stream body as file with name as given in the query parameters. POST /cube Uploads a JSON description of a cube and returns a cube ID. POST /cube/{cube_id}/{chunk_id}/start Queue a chunk read for a given cube and given chunk id as given in the path, returns immediately. GET /cube/{cube_id}/{chunk_id}/status Ask for the status of a chunk read request. Possible return values are notrequested , queued , running , finished , and error . GET /cube/{cube_id}/{chunk_id}/download Download a chunk with given id as application/octet-stream. The chunk must have been queued before. If the chunk has not yet been read, it will block until the data becomes available.","title":"Server API"},{"location":"server-api.html#rest-like-web-api-for-distributed-processing","text":"For distributed processing, gdalcubes and gdalcubes_server communicate via a REST-like API (no HATEOAS). The following documentation describes available endpoints. By default, gdalcubes_server will listen on http://0.0.0.0:1111/gdalcubes/api/ .","title":"REST-like web API for distributed processing"},{"location":"server-api.html#get-version","text":"Returns version information as text","title":"GET /version"},{"location":"server-api.html#head-filenamexyzampsize1234","text":"Checks whether the server instance has file with name and size equal to the given query parameters. Returns 200, if the file is available, 409 if a file with the same name has different size, ...","title":"HEAD /file?name=xyz&amp;size=1234"},{"location":"server-api.html#post-filenamexyz","text":"Uploads the file in the application/octet-stream body as file with name as given in the query parameters.","title":"POST /file?name=xyz"},{"location":"server-api.html#post-cube","text":"Uploads a JSON description of a cube and returns a cube ID.","title":"POST /cube"},{"location":"server-api.html#post-cubecube_idchunk_idstart","text":"Queue a chunk read for a given cube and given chunk id as given in the path, returns immediately.","title":"POST /cube/{cube_id}/{chunk_id}/start"},{"location":"server-api.html#get-cubecube_idchunk_idstatus","text":"Ask for the status of a chunk read request. Possible return values are notrequested , queued , running , finished , and error .","title":"GET /cube/{cube_id}/{chunk_id}/status"},{"location":"server-api.html#get-cubecube_idchunk_iddownload","text":"Download a chunk with given id as application/octet-stream. The chunk must have been queued before. If the chunk has not yet been read, it will block until the data becomes available.","title":"GET /cube/{cube_id}/{chunk_id}/download"}]}